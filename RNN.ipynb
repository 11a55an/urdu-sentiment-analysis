{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%cd '/kaggle/input/urdu-comments-classification'","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:59:32.111785Z","iopub.execute_input":"2024-06-06T08:59:32.112201Z","iopub.status.idle":"2024-06-06T08:59:32.121140Z","shell.execute_reply.started":"2024-06-06T08:59:32.112164Z","shell.execute_reply":"2024-06-06T08:59:32.119966Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/urdu-comments-classification\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:58:14.081657Z","iopub.execute_input":"2024-06-06T08:58:14.082053Z","iopub.status.idle":"2024-06-06T08:58:14.489347Z","shell.execute_reply.started":"2024-06-06T08:58:14.082024Z","shell.execute_reply":"2024-06-06T08:58:14.488159Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_excel(\"train.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:59:45.294316Z","iopub.execute_input":"2024-06-06T08:59:45.294718Z","iopub.status.idle":"2024-06-06T08:59:46.154622Z","shell.execute_reply.started":"2024-06-06T08:59:45.294688Z","shell.execute_reply":"2024-06-06T08:59:46.153734Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test = pd.read_excel(\"test.xlsx\")\nvalid = pd.read_excel(\"valid.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:00:24.143044Z","iopub.execute_input":"2024-06-06T09:00:24.143457Z","iopub.status.idle":"2024-06-06T09:00:24.506936Z","shell.execute_reply.started":"2024-06-06T09:00:24.143428Z","shell.execute_reply":"2024-06-06T09:00:24.505943Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:00:32.240527Z","iopub.execute_input":"2024-06-06T09:00:32.241001Z","iopub.status.idle":"2024-06-06T09:00:32.256680Z","shell.execute_reply.started":"2024-06-06T09:00:32.240967Z","shell.execute_reply":"2024-06-06T09:00:32.255383Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                             comment target\n0  متفق  افراد کو جوڑنا سب کو ضرورت کی جگہ پر  کر...    neu\n1           شاھد بایھی اپ کو ون ڈےکرکٹ میں واپس آجاے    neu\n2                            سر مجھے بھی ایک  دے دیں    neu\n3                شاہد بھاہی جنرل راحیل کی طرح شیر ہے    neu\n4                                     پوں پوں آفریدی    neu","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>متفق  افراد کو جوڑنا سب کو ضرورت کی جگہ پر  کر...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>شاھد بایھی اپ کو ون ڈےکرکٹ میں واپس آجاے</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>سر مجھے بھی ایک  دے دیں</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>شاہد بھاہی جنرل راحیل کی طرح شیر ہے</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>پوں پوں آفریدی</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:00:36.888232Z","iopub.execute_input":"2024-06-06T09:00:36.888693Z","iopub.status.idle":"2024-06-06T09:00:36.899245Z","shell.execute_reply.started":"2024-06-06T09:00:36.888660Z","shell.execute_reply":"2024-06-06T09:00:36.898184Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                             comment target\n0  زمان خان نے آوٹ نہ کر کے اچھا کیا باقی سب گیم ...    neu\n1  اس غریب کے پاس کوئئ سفارش نہیں تبھی اس بیچارے ...    neu\n2  پاکستانی ٹیم میں کمزور پلیئر امام الحق محمد نو...    neu\n3  اچھا تھمب نیل میں لڑکی دکھانا لازمی ہے کیا بات ہے    neu\n4     آپ میری تجویز پر عمل کریں گے  مجھے بڑی امید ھے    neu","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>زمان خان نے آوٹ نہ کر کے اچھا کیا باقی سب گیم ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>اس غریب کے پاس کوئئ سفارش نہیں تبھی اس بیچارے ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>پاکستانی ٹیم میں کمزور پلیئر امام الحق محمد نو...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اچھا تھمب نیل میں لڑکی دکھانا لازمی ہے کیا بات ہے</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>آپ میری تجویز پر عمل کریں گے  مجھے بڑی امید ھے</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"valid.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:00:41.586478Z","iopub.execute_input":"2024-06-06T09:00:41.586884Z","iopub.status.idle":"2024-06-06T09:00:41.596997Z","shell.execute_reply.started":"2024-06-06T09:00:41.586852Z","shell.execute_reply":"2024-06-06T09:00:41.595952Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                             comment target\n0  کچے کیلے یہاں ناروے میں عام دکانوں سے کٹی حالت...    neu\n1  کہیں سے بھی ملیں کیلے بس مل جائیں میں ریکوئسٹ ...    neu\n2  آپ نے بھی سعود بھائی کی ترکیب سے بنائے تھے جاسمن؟    neu\n3  کٹے ہوئے خشک کیلے پکوڑوں کی اس ترکیب کے لیے من...    neu\n4  سعود بھائی اگر آپ وعدہ کریں کہ اسی طرح تفصیل س...    neu","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>کچے کیلے یہاں ناروے میں عام دکانوں سے کٹی حالت...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>کہیں سے بھی ملیں کیلے بس مل جائیں میں ریکوئسٹ ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>آپ نے بھی سعود بھائی کی ترکیب سے بنائے تھے جاسمن؟</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>کٹے ہوئے خشک کیلے پکوڑوں کی اس ترکیب کے لیے من...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>سعود بھائی اگر آپ وعدہ کریں کہ اسی طرح تفصیل س...</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(train.shape,\ntest.shape,\nvalid.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:01:18.034069Z","iopub.execute_input":"2024-06-06T09:01:18.034933Z","iopub.status.idle":"2024-06-06T09:01:18.040198Z","shell.execute_reply.started":"2024-06-06T09:01:18.034899Z","shell.execute_reply":"2024-06-06T09:01:18.039084Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(6399, 2) (2001, 2) (1602, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"train = train.drop_duplicates()\ntest = test.drop_duplicates()\nvalid = valid.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:01:57.706714Z","iopub.execute_input":"2024-06-06T09:01:57.707119Z","iopub.status.idle":"2024-06-06T09:01:57.731242Z","shell.execute_reply.started":"2024-06-06T09:01:57.707086Z","shell.execute_reply":"2024-06-06T09:01:57.730181Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(train.shape,\ntest.shape,\nvalid.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:02:04.750844Z","iopub.execute_input":"2024-06-06T09:02:04.751747Z","iopub.status.idle":"2024-06-06T09:02:04.757005Z","shell.execute_reply.started":"2024-06-06T09:02:04.751710Z","shell.execute_reply":"2024-06-06T09:02:04.755837Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(6335, 2) (1984, 2) (1593, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install urduhack","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:03:28.545838Z","iopub.execute_input":"2024-06-06T09:03:28.546306Z","iopub.status.idle":"2024-06-06T09:03:52.097383Z","shell.execute_reply.started":"2024-06-06T09:03:28.546271Z","shell.execute_reply":"2024-06-06T09:03:52.095995Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting urduhack\n  Downloading urduhack-1.1.1-py3-none-any.whl.metadata (7.2 kB)\nCollecting tf2crf (from urduhack)\n  Downloading tf2crf-0.1.33-py2.py3-none-any.whl.metadata (1.9 kB)\nCollecting tensorflow-datasets~=3.1 (from urduhack)\n  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl.metadata (4.8 kB)\nCollecting Click~=7.1 (from urduhack)\n  Downloading click-7.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from urduhack) (2023.12.25)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.4.0)\nRequirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (23.2.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (0.3.8)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.26.4)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (3.20.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.16.0)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (0.14.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (4.66.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.14.1)\nRequirement already satisfied: tensorflow>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from tf2crf->urduhack) (2.15.0)\nCollecting tensorflow-addons>=0.8.2 (from tf2crf->urduhack)\n  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2024.2.2)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (21.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (69.0.3)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (4.9.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow>=2.1.0->tf2crf->urduhack)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting typeguard<3.0.0,>=2.7 (from tensorflow-addons>=0.8.2->tf2crf->urduhack)\n  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack) (1.62.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->tf2crf->urduhack) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow>=2.1.0->tf2crf->urduhack) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->tf2crf->urduhack) (3.2.2)\nDownloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\nDownloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nInstalling collected packages: typeguard, keras, Click, tensorflow-addons, tensorflow-datasets, tf2crf, urduhack\n  Attempting uninstall: typeguard\n    Found existing installation: typeguard 4.1.5\n    Uninstalling typeguard-4.1.5:\n      Successfully uninstalled typeguard-4.1.5\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: Click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\n  Attempting uninstall: tensorflow-datasets\n    Found existing installation: tensorflow-datasets 4.9.4\n    Uninstalling tensorflow-datasets-4.9.4:\n      Successfully uninstalled tensorflow-datasets-4.9.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ndask 2024.5.2 requires click>=8.1, but you have click 7.1.2 which is incompatible.\nfiona 1.9.6 requires click~=8.0, but you have click 7.1.2 which is incompatible.\nfitter 1.7.0 requires click<9.0.0,>=8.1.6, but you have click 7.1.2 which is incompatible.\nflask 3.0.3 requires click>=8.1.3, but you have click 7.1.2 which is incompatible.\nkfp 2.5.0 requires click<9,>=8.0.0, but you have click 7.1.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires typeguard<5,>=4.1.2, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Click-7.1.2 keras-2.15.0 tensorflow-addons-0.23.0 tensorflow-datasets-3.2.1 tf2crf-0.1.33 typeguard-2.13.3 urduhack-1.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install demoji","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:03:52.100172Z","iopub.execute_input":"2024-06-06T09:03:52.100659Z","iopub.status.idle":"2024-06-06T09:04:06.632878Z","shell.execute_reply.started":"2024-06-06T09:03:52.100611Z","shell.execute_reply":"2024-06-06T09:04:06.631659Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Collecting demoji\n  Downloading demoji-1.1.0-py3-none-any.whl.metadata (9.2 kB)\nDownloading demoji-1.1.0-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: demoji\nSuccessfully installed demoji-1.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import demoji\nimport re\n\nfrom urduhack.preprocessing import replace_urls\nfrom urduhack.preprocessing import replace_numbers\nfrom urduhack.preprocessing import remove_punctuation\nfrom urduhack.preprocessing import remove_accents\nfrom urduhack.preprocessing import remove_english_alphabets\nfrom urduhack.preprocessing import normalize_whitespace\nfrom urduhack.normalization import normalize\n\ndef clean_comment(comment):\n\n    comment = demoji.replace(comment,\"\")\n\n    comment = re.sub(r'@[A-Za-z0-9]+', '', comment)\n\n    comment = re.sub(r'#[A-Za-z0-9]*', '', comment)\n\n    comment = replace_urls(comment,replace_with=\"\")\n\n    comment = replace_numbers(comment,replace_with=\"\")\n\n    comment = remove_accents(comment)\n\n    comment = remove_english_alphabets(comment)\n\n    comment = normalize_whitespace(comment)\n\n    comment = remove_punctuation(comment)\n\n    comment = normalize(comment)\n\n    comment = re.sub(r'\\+', '', comment)\n\n    comment = re.sub(r'\\=', '', comment)\n\n    return comment","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:04:38.592259Z","iopub.execute_input":"2024-06-06T09:04:38.592737Z","iopub.status.idle":"2024-06-06T09:04:51.972437Z","shell.execute_reply.started":"2024-06-06T09:04:38.592700Z","shell.execute_reply":"2024-06-06T09:04:51.971256Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"2024-06-06 09:04:40.494740: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-06 09:04:40.494858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-06 09:04:40.625775: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in train.index:\n    a = train['comment'][i]\n    b = clean_comment(a)\n    train.loc[[i],['comment']] = b\n\nfor i in valid.index:\n    a = valid['comment'][i]\n    b = clean_comment(a)\n    valid.loc[[i],['comment']] = b\n\nfor i in test.index:\n    a = test['comment'][i]\n    b = clean_comment(a)\n    test.loc[[i],['comment']] = b","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:04:59.219745Z","iopub.execute_input":"2024-06-06T09:04:59.220472Z","iopub.status.idle":"2024-06-06T09:05:18.065160Z","shell.execute_reply.started":"2024-06-06T09:04:59.220439Z","shell.execute_reply":"2024-06-06T09:05:18.064254Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"stop_words = set(\"\"\"\n\n آ آئی آئیں آئے آتا آتی آتے آداب آدھ آدھا آدھی آدھے آس آمدید آنا آنسہ آنی آنے\n آپ آگے آہ آہا آیا اب ابھی ابے اتوار ارب اربویں ارے اس اسکا اسکی اسکے اسی اسے اف\n افوہ الاول البتہ الثانی الحرام السلام الف المکرم ان اندر انکا انکی انکے انہوں انہی انہیں اوئے اور\n اوپر اوہو اپ اپنا اپنوں اپنی اپنے اپنےآپ اکبر اکثر اگر اگرچہ اگست اہاہا ایسا ایسی ایسے ایک بائیں\n بار بارے بالکل باوجود باہر بج بجے بخیر برسات بشرطیکہ بعد بعض بغیر بلکہ بن بنا بناؤ بند\n بڑی بھر بھریں بھی بہار بہت بہتر بیگم تاکہ تاہم تب تجھ تجھی تجھے ترا تری تلک تم تمام\n تمہارا تمہاروں تمہاری تمہارے تمہیں تو تک تھا تھی تھیں تھے تہائی تیرا تیری تیرے تین جا جاؤ\n جائیں جائے جاتا جاتی جاتے جانی جانے جب جبکہ جدھر جس جسے جن جناب جنہوں جنہیں جو جہاں جی\n جیسا جیسوں جیسی جیسے جیٹھ حالانکہ حالاں حصہ حضرت خاطر خالی خدا خزاں خواہ خوب خود دائیں درمیان\n دریں دو دوران دوسرا دوسروں دوسری دوشنبہ دوں دکھائیں دگنا دی دیئے دیا دیتا دیتی دیتے دیر دینا دینی\n دینے دیکھو دیں دیے دے ذریعے رکھا رکھتا رکھتی رکھتے رکھنا رکھنی رکھنے رکھو رکھی رکھے رہ رہا\n رہتا رہتی رہتے رہنا رہنی رہنے رہو رہی رہیں رہے ساتھ سامنے ساڑھے سب سبھی سراسر سلام سمیت سوا\n سوائے سکا سکتا سکتے سہ سہی سی سے شام شاید شکریہ صاحب صاحبہ صرف ضرور طرح طرف طور\n علاوہ عین فروری فقط فلاں فی قبل قطا لئے لائی لائے لاتا لاتی لاتے لانا لانی لانے لایا لو\n لوجی لوگوں لگ لگا لگتا لگتی لگی لگیں لگے لہذا لی لیا لیتا لیتی لیتے لیکن لیں لیے\n لے ماسوا مت مجھ مجھی مجھے محترم محترمہ محترمی محض مرا مرحبا مری مرے مزید مس مسز مسٹر مطابق\n مطلق مل منٹ منٹوں مکرمی مگر مگھر مہربانی میرا میروں میری میرے میں نا نزدیک نما نو نومبر\n نہ نہیں نیز نیچے نے و وار واسطے واقعی والا والوں والی والے واہ وجہ ورنہ وعلیکم وغیرہ ولے\n وگرنہ وہ وہاں وہی وہیں ویسا ویسے ویں پاس پایا پر پس پلیز پون پونا پونی پونے پھاگن\n پھر پہ پہر پہلا پہلی پہلے پیر پیچھے چاہئے چاہتے چاہیئے چاہے چلا چلو چلیں چلے چناچہ چند چونکہ\n چوگنی چکی چکیں چکے چہارشنبہ چیت ڈالنا ڈالنی ڈالنے ڈالے کئے کا کاتک کاش کب کبھی کدھر کر\n کرتا کرتی کرتے کرم کرنا کرنے کرو کریں کرے کس کسی کسے کل کم کن کنہیں کو کوئی کون\n کونسا کونسے کچھ کہ کہا کہاں کہہ کہی کہیں کہے کی کیا کیسا کیسے کیونکر کیونکہ کیوں کیے\n کے گئی گئے گا گرما گرمی گنا گو گویا گھنٹا گھنٹوں گھنٹے گی گیا ہائیں ہائے ہاڑ ہاں ہر\n ہرچند ہرگز ہزار ہفتہ ہم ہمارا ہماری ہمارے ہمی ہمیں ہو ہوئی ہوئیں ہوئے ہوا ہوبہو ہوتا ہوتی\n ہوتیں ہوتے ہونا ہونگے ہونی ہونے ہوں ہی ہیلو ہیں ہے یا یات یعنی یک یہ یہاں یہی یہیں\n\n\n\"\"\".split())\n\ndef remove_stopwords(text):\n    filtered_words = [word for word in text.split() if word not in stop_words]\n    return \" \".join(filtered_words)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:05:35.086441Z","iopub.execute_input":"2024-06-06T09:05:35.087035Z","iopub.status.idle":"2024-06-06T09:05:35.097083Z","shell.execute_reply.started":"2024-06-06T09:05:35.087000Z","shell.execute_reply":"2024-06-06T09:05:35.096037Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train[\"comment\"] = train.comment.map(remove_stopwords)\nvalid[\"comment\"] = valid.comment.map(remove_stopwords)\ntest[\"comment\"] = test.comment.map(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:05:43.176748Z","iopub.execute_input":"2024-06-06T09:05:43.177168Z","iopub.status.idle":"2024-06-06T09:05:43.241510Z","shell.execute_reply.started":"2024-06-06T09:05:43.177136Z","shell.execute_reply":"2024-06-06T09:05:43.240563Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import urduhack\nurduhack.download()\nfrom urduhack.models.lemmatizer import lemmatizer\n\ndef lemitizeStr(text):\n    lemme_txt = \"\"\n    temp = lemmatizer.lemma_lookup(text)\n    for t in temp:\n        lemme_txt += t[0] + \" \"\n\n    return lemme_txt","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:08:14.207064Z","iopub.execute_input":"2024-06-06T09:08:14.207831Z","iopub.status.idle":"2024-06-06T09:08:17.122080Z","shell.execute_reply.started":"2024-06-06T09:08:14.207794Z","shell.execute_reply":"2024-06-06T09:08:17.121030Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Downloading data from https://github.com/urduhack/resources/releases/download/word_tokenizer/word_tokenizer.zip\n36788015/36788015 [==============================] - 0s 0us/step\nDownloading data from https://github.com/urduhack/resources/releases/download/pos_tagger/pos_tagger.zip\n2761433/2761433 [==============================] - 0s 0us/step\nDownloading data from https://github.com/urduhack/resources/releases/download/ner/ner.zip\n11723346/11723346 [==============================] - 0s 0us/step\nDownloading data from https://github.com/urduhack/resources/releases/download/lemmatizer/ur_lemma_lookup.zip\n89078/89078 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"train[\"comment\"] = train.comment.map(lemitizeStr)\nvalid[\"comment\"] = valid.comment.map(lemitizeStr)\ntest[\"comment\"] = test.comment.map(lemitizeStr)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:08:18.649827Z","iopub.execute_input":"2024-06-06T09:08:18.650238Z","iopub.status.idle":"2024-06-06T09:08:18.747463Z","shell.execute_reply.started":"2024-06-06T09:08:18.650207Z","shell.execute_reply":"2024-06-06T09:08:18.746491Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:11:26.573810Z","iopub.execute_input":"2024-06-06T09:11:26.574247Z","iopub.status.idle":"2024-06-06T09:11:26.592288Z","shell.execute_reply.started":"2024-06-06T09:11:26.574215Z","shell.execute_reply":"2024-06-06T09:11:26.585861Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"target\nneg    2441\nneu    2026\npos    1868\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"test['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:11:40.274455Z","iopub.execute_input":"2024-06-06T09:11:40.275415Z","iopub.status.idle":"2024-06-06T09:11:40.283821Z","shell.execute_reply.started":"2024-06-06T09:11:40.275380Z","shell.execute_reply":"2024-06-06T09:11:40.282787Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"target\nneg    769\nneu    632\npos    583\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"valid['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:11:47.056709Z","iopub.execute_input":"2024-06-06T09:11:47.057422Z","iopub.status.idle":"2024-06-06T09:11:47.065841Z","shell.execute_reply.started":"2024-06-06T09:11:47.057391Z","shell.execute_reply":"2024-06-06T09:11:47.064992Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"target\nneg    616\nneu    504\npos    473\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nclass_label_encoder = LabelEncoder()\n\ntrain['target_encoded'] = class_label_encoder.fit_transform(train['target'])\ntest['target_encoded'] = class_label_encoder.fit_transform(test['target'])\nvalid['target_encoded'] = class_label_encoder.fit_transform(valid['target'])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:35.322951Z","iopub.execute_input":"2024-06-06T09:13:35.323405Z","iopub.status.idle":"2024-06-06T09:13:35.693533Z","shell.execute_reply.started":"2024-06-06T09:13:35.323375Z","shell.execute_reply":"2024-06-06T09:13:35.692640Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train['target_encoded'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:52.900371Z","iopub.execute_input":"2024-06-06T09:13:52.900838Z","iopub.status.idle":"2024-06-06T09:13:52.911539Z","shell.execute_reply.started":"2024-06-06T09:13:52.900803Z","shell.execute_reply":"2024-06-06T09:13:52.910395Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"target_encoded\n0    2441\n1    2026\n2    1868\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"x_train = train[\"comment\"]\nx_test = test[\"comment\"]\nx_valid = valid[\"comment\"]\ny_train = train[[\"target_encoded\"]]\ny_test = test[[\"target_encoded\"]]\ny_valid = valid[[\"target_encoded\"]]","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:15:05.831720Z","iopub.execute_input":"2024-06-06T09:15:05.832560Z","iopub.status.idle":"2024-06-06T09:15:05.842283Z","shell.execute_reply.started":"2024-06-06T09:15:05.832523Z","shell.execute_reply":"2024-06-06T09:15:05.841215Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train,valid,test])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:17:41.305857Z","iopub.execute_input":"2024-06-06T09:17:41.306471Z","iopub.status.idle":"2024-06-06T09:17:41.314274Z","shell.execute_reply.started":"2024-06-06T09:17:41.306429Z","shell.execute_reply":"2024-06-06T09:17:41.313186Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:17:46.456492Z","iopub.execute_input":"2024-06-06T09:17:46.456938Z","iopub.status.idle":"2024-06-06T09:17:46.464012Z","shell.execute_reply.started":"2024-06-06T09:17:46.456905Z","shell.execute_reply":"2024-06-06T09:17:46.462832Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(9912, 3)"},"metadata":{}}]},{"cell_type":"code","source":"from collections import Counter\n\ndef counter_word(text_col):\n    count = Counter()\n    for text in text_col.values:\n        for word in text.split():\n            count[word] += 1\n    return count","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:17:50.439239Z","iopub.execute_input":"2024-06-06T09:17:50.439688Z","iopub.status.idle":"2024-06-06T09:17:50.445710Z","shell.execute_reply.started":"2024-06-06T09:17:50.439653Z","shell.execute_reply":"2024-06-06T09:17:50.444605Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"counter = counter_word(df.comment)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:17:54.109438Z","iopub.execute_input":"2024-06-06T09:17:54.110509Z","iopub.status.idle":"2024-06-06T09:17:54.158516Z","shell.execute_reply.started":"2024-06-06T09:17:54.110463Z","shell.execute_reply":"2024-06-06T09:17:54.157384Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"num_unique_words = len(counter)\nnum_unique_words","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:17:55.359323Z","iopub.execute_input":"2024-06-06T09:17:55.359850Z","iopub.status.idle":"2024-06-06T09:17:55.367809Z","shell.execute_reply.started":"2024-06-06T09:17:55.359800Z","shell.execute_reply":"2024-06-06T09:17:55.366520Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"12362"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=num_unique_words)\ntokenizer.fit_on_texts(x_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:19:17.170620Z","iopub.execute_input":"2024-06-06T09:19:17.171034Z","iopub.status.idle":"2024-06-06T09:19:17.326814Z","shell.execute_reply.started":"2024-06-06T09:19:17.171003Z","shell.execute_reply":"2024-06-06T09:19:17.325656Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:19:31.028537Z","iopub.execute_input":"2024-06-06T09:19:31.029505Z","iopub.status.idle":"2024-06-06T09:19:31.033975Z","shell.execute_reply.started":"2024-06-06T09:19:31.029470Z","shell.execute_reply":"2024-06-06T09:19:31.032916Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"len(word_index)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:19:57.903671Z","iopub.execute_input":"2024-06-06T09:19:57.904082Z","iopub.status.idle":"2024-06-06T09:19:57.911207Z","shell.execute_reply.started":"2024-06-06T09:19:57.904050Z","shell.execute_reply":"2024-06-06T09:19:57.909956Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"9175"},"metadata":{}}]},{"cell_type":"code","source":"X_train_seq = tokenizer.texts_to_sequences(x_train)\nX_valid_seq = tokenizer.texts_to_sequences(x_valid)\nX_test_seq = tokenizer.texts_to_sequences(x_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:20:35.143908Z","iopub.execute_input":"2024-06-06T09:20:35.144912Z","iopub.status.idle":"2024-06-06T09:20:35.342030Z","shell.execute_reply.started":"2024-06-06T09:20:35.144869Z","shell.execute_reply":"2024-06-06T09:20:35.341085Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_length = 20\nX_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding=\"post\")\nX_valid_padded = pad_sequences(X_valid_seq, maxlen=max_length, padding=\"post\")\nX_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding=\"post\")\nX_train_padded.shape, X_valid_padded.shape, X_test_padded.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:22:23.346193Z","iopub.execute_input":"2024-06-06T09:22:23.346845Z","iopub.status.idle":"2024-06-06T09:22:23.408787Z","shell.execute_reply.started":"2024-06-06T09:22:23.346813Z","shell.execute_reply":"2024-06-06T09:22:23.407671Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"((6335, 20), (1593, 20), (1984, 20))"},"metadata":{}}]},{"cell_type":"code","source":"print(x_train.iloc[10])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:22:40.828667Z","iopub.execute_input":"2024-06-06T09:22:40.829102Z","iopub.status.idle":"2024-06-06T09:22:40.835086Z","shell.execute_reply.started":"2024-06-06T09:22:40.829068Z","shell.execute_reply":"2024-06-06T09:22:40.833883Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"آج ارجنٹینا میچ دیکھ ارجنٹینا جرمنی رکھوں ارجنٹینا برازیل ٹیم فاتح باقی انگلیڈ کھیل مایوس رونی صورتحال سنبھال \n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_train_seq[10])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:22:47.981493Z","iopub.execute_input":"2024-06-06T09:22:47.981917Z","iopub.status.idle":"2024-06-06T09:22:47.987540Z","shell.execute_reply.started":"2024-06-06T09:22:47.981888Z","shell.execute_reply":"2024-06-06T09:22:47.986415Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"[22, 1256, 3, 37, 1256, 491, 2759, 1256, 794, 2, 524, 149, 4055, 16, 664, 2114, 990, 2115]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_train_padded[10])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:22:55.590013Z","iopub.execute_input":"2024-06-06T09:22:55.590400Z","iopub.status.idle":"2024-06-06T09:22:55.596423Z","shell.execute_reply.started":"2024-06-06T09:22:55.590372Z","shell.execute_reply":"2024-06-06T09:22:55.595254Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"[  22 1256    3   37 1256  491 2759 1256  794    2  524  149 4055   16\n  664 2114  990 2115    0    0]\n","output_type":"stream"}]},{"cell_type":"code","source":"weight_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:33:26.809830Z","iopub.execute_input":"2024-06-06T09:33:26.810347Z","iopub.status.idle":"2024-06-06T09:33:26.820076Z","shell.execute_reply.started":"2024-06-06T09:33:26.810306Z","shell.execute_reply":"2024-06-06T09:33:26.818556Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"(12362, 300)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False)\n\n# Fit and transform the data\nencoded_array = encoder.fit_transform(train[['target']])\n\n# Create a DataFrame with the encoded data\ntrain_encoded = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out())\n\n# Fit and transform the data\nencoded_array = encoder.fit_transform(valid[['target']])\n\n# Create a DataFrame with the encoded data\nvalid_encoded = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out())\n\n# Fit and transform the data\nencoded_array = encoder.fit_transform(test[['target']])\n\n# Create a DataFrame with the encoded data\ntest_encoded = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:46:36.655688Z","iopub.execute_input":"2024-06-06T09:46:36.656259Z","iopub.status.idle":"2024-06-06T09:46:36.680411Z","shell.execute_reply.started":"2024-06-06T09:46:36.656219Z","shell.execute_reply":"2024-06-06T09:46:36.679213Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"y_train, y_valid, y_test = train_encoded, valid_encoded, test_encoded","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:47:12.293688Z","iopub.execute_input":"2024-06-06T09:47:12.294227Z","iopub.status.idle":"2024-06-06T09:47:12.299988Z","shell.execute_reply.started":"2024-06-06T09:47:12.294186Z","shell.execute_reply":"2024-06-06T09:47:12.298779Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nword2vec = KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin',  binary=True, limit=10 ** 5)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:32:43.991271Z","iopub.execute_input":"2024-06-06T09:32:43.991793Z","iopub.status.idle":"2024-06-06T09:32:46.889504Z","shell.execute_reply.started":"2024-06-06T09:32:43.991753Z","shell.execute_reply":"2024-06-06T09:32:46.888322Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nembedding_dim = 300\nweight_matrix = np.zeros((num_unique_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    try:\n        embedding_vector = word2vec[word]\n        weight_matrix[i] = embedding_vector\n    except KeyError:\n        weight_matrix[i] = np.random.uniform(-5, 5, embedding_dim)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:33:18.561240Z","iopub.execute_input":"2024-06-06T09:33:18.561780Z","iopub.status.idle":"2024-06-06T09:33:18.719056Z","shell.execute_reply.started":"2024-06-06T09:33:18.561736Z","shell.execute_reply":"2024-06-06T09:33:18.717990Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Bidirectional, SimpleRNN, Activation, LSTM, GRU, Embedding, Bidirectional, InputLayer, Conv1D, GlobalMaxPooling1D, Flatten\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nbatch_size=32\n\nmodel = keras.models.Sequential()\nmodel.add(layers.Embedding(num_unique_words, embedding_dim, weights=[weight_matrix], input_length=max_length))\n\n\n\n# model.add(GRU(200, return_sequences=True))\n# model.add(Dropout(0.3))\n# model.add(GRU(200, return_sequences=True))\n# model.add(Dropout(0.3))\n# model.add(GRU(200, return_sequences=False))\n# model.add(Dropout(0.3))\n\n#     model.add(Conv1D(128, 5, activation='relu'))  # Convolutional layer\n#     model.add(GlobalMaxPooling1D())\n#     model.add(Conv1D(128, 5, activation='relu'))  # Convolutional layer\n#     model.add(GlobalMaxPooling1D())\n#     model.add(Dropout(0.3))\n#     model.add(Conv1D(128, 5, activation='relu'))  # Convolutional layer\n#     model.add(Dropout(0.3))\n#     model.add(GlobalMaxPooling1D())\n#     model.add(Dense(64))\n\nmodel.add(Bidirectional(LSTM(200, return_sequences=True)))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(200, return_sequences=True)))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(200)))\nmodel.add(Dropout(0.3))\n\n# model.add(SimpleRNN(200, return_sequences=True))\n# model.add(Dropout(0.3))\n# model.add(SimpleRNN(200, return_sequences=True))\n# model.add(Dropout(0.3))\n# model.add(SimpleRNN(200))\n# model.add(Dropout(0.3))\n\n# model.add(Flatten())\n# model.add(Dense(500,activation='relu'))\n# model.add(Dropout(0.3))\n# model.add(Dense(500,activation='relu'))\n# model.add(Dropout(0.3))\n# model.add(Dense(500,activation='relu'))\n# model.add(Dropout(0.3))\n\n\n# model.add(LSTM(128, return_sequences=True))\n# model.add(Dropout(0.3))\n# model.add(LSTM(64, return_sequences=True))\n# model.add(Dropout(0.3))\n# model.add(LSTM(32))\n# model.add(Dropout(0.3))\n\n\nmodel.add(Dense(3, activation='softmax'))\nmodel.summary()\n\nloss = keras.losses.CategoricalCrossentropy(from_logits=False)\noptim = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss=loss, optimizer=optim)\n\nmodel.fit(X_train_padded, y_train, validation_data = (X_valid_padded,y_valid),batch_size = batch_size, epochs=15, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T10:50:22.660692Z","iopub.execute_input":"2024-06-06T10:50:22.661467Z","iopub.status.idle":"2024-06-06T11:02:59.363579Z","shell.execute_reply.started":"2024-06-06T10:50:22.661430Z","shell.execute_reply":"2024-06-06T11:02:59.362550Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"Model: \"sequential_28\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_28 (Embedding)    (None, 20, 300)           3708600   \n                                                                 \n bidirectional (Bidirection  (None, 20, 400)           801600    \n al)                                                             \n                                                                 \n dropout_77 (Dropout)        (None, 20, 400)           0         \n                                                                 \n bidirectional_1 (Bidirecti  (None, 20, 400)           961600    \n onal)                                                           \n                                                                 \n dropout_78 (Dropout)        (None, 20, 400)           0         \n                                                                 \n bidirectional_2 (Bidirecti  (None, 400)               961600    \n onal)                                                           \n                                                                 \n dropout_79 (Dropout)        (None, 400)               0         \n                                                                 \n dense_52 (Dense)            (None, 3)                 1203      \n                                                                 \n=================================================================\nTotal params: 6434603 (24.55 MB)\nTrainable params: 6434603 (24.55 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/15\n198/198 - 84s - loss: 1.0876 - val_loss: 1.0593 - 84s/epoch - 427ms/step\nEpoch 2/15\n198/198 - 49s - loss: 0.9225 - val_loss: 1.0891 - 49s/epoch - 246ms/step\nEpoch 3/15\n198/198 - 48s - loss: 0.8190 - val_loss: 1.0897 - 48s/epoch - 244ms/step\nEpoch 4/15\n198/198 - 48s - loss: 0.7630 - val_loss: 1.2047 - 48s/epoch - 241ms/step\nEpoch 5/15\n198/198 - 47s - loss: 0.7436 - val_loss: 1.1657 - 47s/epoch - 238ms/step\nEpoch 6/15\n198/198 - 48s - loss: 0.7143 - val_loss: 1.1973 - 48s/epoch - 241ms/step\nEpoch 7/15\n198/198 - 48s - loss: 0.7241 - val_loss: 1.2592 - 48s/epoch - 240ms/step\nEpoch 8/15\n198/198 - 48s - loss: 0.7467 - val_loss: 1.2161 - 48s/epoch - 242ms/step\nEpoch 9/15\n198/198 - 47s - loss: 0.8399 - val_loss: 1.1531 - 47s/epoch - 239ms/step\nEpoch 10/15\n198/198 - 48s - loss: 0.8937 - val_loss: 1.1527 - 48s/epoch - 242ms/step\nEpoch 11/15\n198/198 - 48s - loss: 0.9274 - val_loss: 1.1001 - 48s/epoch - 244ms/step\nEpoch 12/15\n198/198 - 48s - loss: 0.9443 - val_loss: 1.0780 - 48s/epoch - 244ms/step\nEpoch 13/15\n198/198 - 47s - loss: 0.9676 - val_loss: 1.1352 - 47s/epoch - 240ms/step\nEpoch 14/15\n198/198 - 48s - loss: 1.0185 - val_loss: 1.1294 - 48s/epoch - 243ms/step\nEpoch 15/15\n198/198 - 48s - loss: 1.0549 - val_loss: 1.0948 - 48s/epoch - 240ms/step\n","output_type":"stream"},{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x79a9f82dd1e0>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_pred_probs = model.predict(X_test_padded)\n\n# Convert predictions to class labels\ny_pred = np.argmax(y_pred_probs, axis=-1)\n\n# Convert test labels to class labels\ny_true = np.argmax(y_test.to_numpy(), axis=-1)\n\n# Calculating accuracy\naccuracy = accuracy_score(y_true, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n# Generating classification report\nreport = classification_report(y_true, y_pred)\nprint('Classification Report:')\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T11:03:05.347790Z","iopub.execute_input":"2024-06-06T11:03:05.349767Z","iopub.status.idle":"2024-06-06T11:03:11.900820Z","shell.execute_reply.started":"2024-06-06T11:03:05.349729Z","shell.execute_reply":"2024-06-06T11:03:11.899739Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"62/62 [==============================] - 6s 68ms/step\nAccuracy: 0.4375\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.51      0.47      0.49       769\n           1       0.38      0.45      0.41       632\n           2       0.42      0.38      0.40       583\n\n    accuracy                           0.44      1984\n   macro avg       0.44      0.43      0.43      1984\nweighted avg       0.44      0.44      0.44      1984\n\n","output_type":"stream"}]}]}